{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Articles Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URnavUlDuDt",
        "colab_type": "text"
      },
      "source": [
        "#Import the necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ysNLsDaIHvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.tokenize import WordPunctTokenizer,word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36zm50yDDzLH",
        "colab_type": "text"
      },
      "source": [
        "#Load the Dataset\n",
        "\n",
        "The dataset is uploaded on the GitHub repository and is fetched directly. To access the content for the articles directly, the index is set to the 'Heading' of the article which helps in accessing the text by using the corresponding heading of the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvcDYS9IJI9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url=\"https://raw.githubusercontent.com/Aditi2806/Articles-Dataset/master/Articles%20Dataset.csv\"\n",
        "dataframe=pd.read_csv(url,index_col=\"Heading\",encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT2dJhFZEQnK",
        "colab_type": "text"
      },
      "source": [
        "#Pre-processing the text\n",
        "\n",
        "The text needs to cleaned before taking it as an input. The text is removed of stopwords, punctuations, and other errors. They are tokenized into individual words and n-grams are created to better understand the text. The function 'ngram' is used to implement the same. The function returns a list of the created ngrams which is used to act as the input to our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOx-6QwJfVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram(temp_pos):\n",
        "    '''create positive and negative bigram and trigram models for both category of reviews'''\n",
        "    bigram_pos=gensim.models.Phrases(temp_pos,min_count=1,threshold=1)                  \n",
        "    trigram_pos=gensim.models.Phrases(bigram_pos[temp_pos],min_count=1,threshold=1)     \n",
        "    bigram_pos_mod=gensim.models.phrases.Phraser(bigram_pos)                            #computes the bigrams present in the list\n",
        "    trigram_pos_mod=gensim.models.phrases.Phraser(trigram_pos)                          #computes the trigrams taking the computed bigrams as input\n",
        "\n",
        "    return [trigram_pos_mod[bigram_pos_mod[doc]] for doc in temp_pos]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG_en4-3JioB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lda(temp_pos):\n",
        "    '''defining necessary parameters to LDA model'''\n",
        "\n",
        "    id2word=corpora.Dictionary(temp_pos)                                                                    \n",
        "    texts=temp_pos                                                                                          #create the corpus of the text\n",
        "    corpus=[id2word.doc2bow(text) for text in texts]                                                        #convert the dictionary created above to bag of words form\n",
        "    lda_model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word,num_topics=3,passes=10)\n",
        "    _,pos_keywords=zip(*(lda_model.print_topics()))                                                         \n",
        "    pos_keywords=[y for x in pos_keywords for y in x.split(\"\\\"\") if y !='' and y[0].isalpha()]\n",
        "    pos_keywords=list(set(pos_keywords))                                                                    #creating the final positive keywords list\n",
        "    return pos_keywords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzWZ_K04FIVJ",
        "colab_type": "text"
      },
      "source": [
        "#NLTK Algorithm\n",
        "\n",
        "The function '_create_frequency_table' takes in input the text and applied pre-processing to it. It tokenized the text into words and cleans it to remove stopwords and other terms that add little meaning to the overall text. This function is used to create a frequency table which holds the frequencies of various words present in the text. It tells about the occurrence of each distinct word, which acts as the base of the algorithm. These frequencies are used to score the sentences based upon the number of high frequency words appearing in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYQLjw_oDiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_frequency_table(text_string) -> dict:\n",
        "\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skTEll3EF8A-",
        "colab_type": "text"
      },
      "source": [
        "#Scoring the sentences\n",
        "\n",
        ">The below function takes in the generated frequency table to calculate the scores for every sentence. To rank the sentences, there must be a value that will be used to score the sentences. The sentences are scored based upon the words and their corresponding frequencies in the frequency table. The overall score is calculated by dividing the sum of individual word values with the total number of words present in the sentence.\n",
        "\n",
        ">The '_find_average_score' function takes in the calculated sentence scores to find an average score value. This value is used to set the threshold value for deciding whether to keep a sentence in summary or to discard it. Any sentence having sentence score above this threshold are accepted and those having values less than threshold are rejected and not included in the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqqoHVc8oJMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "{sentence1:score, sentence2:score, sentence3: score,...........}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnHfUQpDoPLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _find_average_score(sentenceValue) -> int:\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original text\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "    return average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6g6nRC6HFLQ",
        "colab_type": "text"
      },
      "source": [
        "#Generating the final summary\n",
        "\n",
        "This function return the final output generated based upon the sentences and their scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P1m4sk1oSkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lra1LZEaHQYc",
        "colab_type": "text"
      },
      "source": [
        "#Executing the algorithm on the dataset\n",
        "\n",
        ">The algorithm first creates a list of distinct article heading present in the text to iterate over the dataset easily. Each article is processed at a time and a summary for the same will be generated. Tokenizing algorithms:'Word probability' and 'Tf-Idf' are applied onto the text to create the vector representation of the text to be fed in as input. The word probability method work supon the frequency for a word and plots the 30 most common words present in the text. The Tf-Idf method calculates a tf-idf score for the word and takes an average of all the tf-idf scores for a particular word at the end to provide scores to every word in the text. The graph of words having highest tf-idf scores is plotted using matplotlib.\n",
        "\n",
        ">These representations are fed into our functions to score the sentences using the above defined functions and create the final summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhnVIFvTJm3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_name=set([i for i in dataframe.index])\n",
        "tokenizer=WordPunctTokenizer()\n",
        "dataframe['NLTK Summary']=\"\"\n",
        "nltk_summary=[]\n",
        "for row in article_name:\n",
        "    word_list=[]\n",
        "    print(\"\\n\\n\\nArticle Heading:\\t\",row)     \n",
        "    print(\"\\nContent:\",dataframe['Content'][row])                                                            \n",
        "    \n",
        "    \n",
        "    word_list_tfidf=[]\n",
        "    tfidf_dict={}\n",
        "    tfidf=TfidfVectorizer(min_df=1,max_df=0.5,ngram_range=(1,3))\n",
        "\n",
        "    try:\n",
        "        \n",
        "        '''word probability'''\n",
        "        token=tokenizer.tokenize(dataframe['Content'][row].lower())\n",
        "        for tokens in token:\n",
        "            if (tokens in stopwords.words('english')) or (tokens in [\",\",\".\",\"'\",\"``\",\"''\",\";\",\"?\",\"--\",\")\",\"(\",\":\",\"!\",\"\\\"\",\"\\'\",\"/\"]):\n",
        "                token.remove(tokens)\n",
        "        word_list = word_list+token\n",
        "\n",
        "        '''Tf-Idf Method'''\n",
        "        word_list_tfidf=nltk.tokenize.sent_tokenize(dataframe['Content'][row])\n",
        "\n",
        "        '''extraction of positive keywords from positive reviews list and negative keywords from negative reviews list'''\n",
        "        temp=[]\n",
        "        c=gensim.utils.simple_preprocess(dataframe['Content'][row],deacc=True)\n",
        "        c=[word for word in c if word not in stopwords.words('english')]       \n",
        "        temp.append(c)                                                      \n",
        "        \n",
        "        '''create positive and negative bigram and trigram models for both category of reviews'''\n",
        "        temp=ngram(temp)                                                        \n",
        "\n",
        "        '''to start the topic modelling using LDA model'''\n",
        "        keywords=lda(temp)\n",
        "\n",
        "        print(\"\\n\\nKeywords extracted from the text are: \",keywords)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "        '''Computing and Plotting Word Probability Scores'''\n",
        "        frequency = nltk.FreqDist(word_list)\n",
        "        word_prob_words,_=zip(*(frequency.most_common(30)))\n",
        "        word_prob_words=list(word_prob_words)\n",
        "        plt.figure(figsize=(5,5))\n",
        "        frequency.plot(30,title='Word Probability Method')\n",
        "\n",
        "        '''Computing and Plotting Tf-Idf scores'''\n",
        "        features=tfidf.fit_transform(word_list_tfidf)\n",
        "        tfidf_data=pd.DataFrame(features.toarray(),columns=tfidf.get_feature_names())\n",
        "\n",
        "        for i in tfidf_data.index:\n",
        "            for x,y in zip(tfidf_data.iloc[i,:],tfidf.get_feature_names()):\n",
        "                if x!=0:\n",
        "                    if y in tfidf_dict.keys():\n",
        "                        tfidf_dict[y]=(tfidf_dict[y]+x)/2\n",
        "                    else:\n",
        "                        tfidf_dict[y]=x\n",
        "\n",
        "        sorted_c= sorted(tfidf_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        x,y= (zip(*sorted_c))\n",
        "        top_x=x[:30]\n",
        "        top_y=y[:30]\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.plot(top_x,top_y)\n",
        "        plt.xticks(top_x,top_x,rotation='vertical')\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel('Tf-Idf Score')\n",
        "        plt.title('TF-IDF Method')\n",
        "        plt.show()\n",
        "        \n",
        "        text = dataframe['Content'][row]\n",
        "        freq_table = _create_frequency_table(text)\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_scores = _score_sentences(sentences, freq_table)\n",
        "\n",
        "        threshold = _find_average_score(sentence_scores)\n",
        "        summary = _generate_summary(sentences, sentence_scores, 1.5 * threshold)\n",
        "        dataframe['NLTK Summary'][row]=summary\n",
        "        print(\"\\n\\nGenerated Summary according to the text:\\n \",summary)\n",
        "\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVft71GJ8Az",
        "colab_type": "code",
        "outputId": "a69a8caf-dc4c-40f1-a9b6-545e8d0f6f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "dataframe[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Content</th>\n",
              "      <th>NLTK Summary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heading</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9 Tips For Training Lightning-Fast Neural Networks In Pytorch</th>\n",
              "      <td>0</td>\n",
              "      <td>Let’s face it, your model is probably still st...</td>\n",
              "      <td>(yup, that just happened). ). Set Trainer(pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>How To Become A One-Drink Wonder</th>\n",
              "      <td>1</td>\n",
              "      <td>Anyone can publish on Medium per our Policies,...</td>\n",
              "      <td>It’s quite a conundrum.The answer? My friend,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Treat Yourself Like a CEO and You’ll Make 10x More Income</th>\n",
              "      <td>2</td>\n",
              "      <td>As I wrote that headline, an old joke came to ...</td>\n",
              "      <td>Friendly eyes. Warm handshake. Raised chin, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bored? 7 Fun Things You Can Build</th>\n",
              "      <td>3</td>\n",
              "      <td>There is no real secret when it comes to becom...</td>\n",
              "      <td>Unfortunately, there are no shortcuts. You’ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>First AI Model of the Universe Knows Science it was Never Taught</th>\n",
              "      <td>4</td>\n",
              "      <td>A new 3D model of the Universe developed by an...</td>\n",
              "      <td>A new 3D model of the Universe developed by a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10 Bad Habits of Unsuccessful People</th>\n",
              "      <td>5</td>\n",
              "      <td>The first successful person I ever met — truly...</td>\n",
              "      <td>“Then 20. Do you have a goal to get healthier...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amazon Accidentally Sent Out Their Email Template</th>\n",
              "      <td>6</td>\n",
              "      <td>It’s comforting to see that even the titans of...</td>\n",
              "      <td>As the template states, the headline must sum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>When Women ‘Dangle the Steak' in Front of Men</th>\n",
              "      <td>7</td>\n",
              "      <td>I truly thought that by now, there wasn’t an o...</td>\n",
              "      <td>I was sexually assaulted by my boyfriend at t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Why Do Men’s Legacies Matter More Than Women’s Safety?</th>\n",
              "      <td>8</td>\n",
              "      <td>Almost immediately after Washington Post repor...</td>\n",
              "      <td>Almost immediately after Washington Post repo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is A.I. the Antichrist?</th>\n",
              "      <td>9</td>\n",
              "      <td>It may seem that old religious principles woul...</td>\n",
              "      <td>I went down the rabbit hole, watching videos ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Unnamed: 0  ...                                       NLTK Summary\n",
              "Heading                                                         ...                                                   \n",
              "9 Tips For Training Lightning-Fast Neural Netwo...           0  ...   (yup, that just happened). ). Set Trainer(pre...\n",
              "How To Become A One-Drink Wonder                             1  ...   It’s quite a conundrum.The answer? My friend,...\n",
              "Treat Yourself Like a CEO and You’ll Make 10x M...           2  ...   Friendly eyes. Warm handshake. Raised chin, s...\n",
              "Bored? 7 Fun Things You Can Build                            3  ...   Unfortunately, there are no shortcuts. You’ve...\n",
              "First AI Model of the Universe Knows Science it...           4  ...   A new 3D model of the Universe developed by a...\n",
              "10 Bad Habits of Unsuccessful People                         5  ...   “Then 20. Do you have a goal to get healthier...\n",
              "Amazon Accidentally Sent Out Their Email Template            6  ...   As the template states, the headline must sum...\n",
              "When Women ‘Dangle the Steak' in Front of Men                7  ...   I was sexually assaulted by my boyfriend at t...\n",
              "Why Do Men’s Legacies Matter More Than Women’s ...           8  ...   Almost immediately after Washington Post repo...\n",
              "Is A.I. the Antichrist?                                      9  ...   I went down the rabbit hole, watching videos ...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6fM2i2zIkTZ",
        "colab_type": "text"
      },
      "source": [
        "#Downloading the Result\n",
        "\n",
        "The summaries generated are stored in a dataframe and converted into an excel file named 'NLTK Result.xlsx'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SYbK-PTAyHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe.to_excel('NLTK Result.xlsx',encoding='utf8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkNDZesyBJmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('NLTK Result.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}